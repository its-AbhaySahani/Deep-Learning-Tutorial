{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6edc774",
   "metadata": {},
   "source": [
    "\n",
    "# Compute Average of Numbers in an RDD using PySpark\n",
    "\n",
    "This notebook demonstrates how to calculate the average of a list of numbers using PySpark. \n",
    "The function `compute_average` takes a list of numbers, parallelizes it into an RDD, \n",
    "maps it into pairs for sum and count, reduces these pairs, and finally computes the average.\n",
    "\n",
    "## Steps Involved\n",
    "1. Parallelize the list into an RDD.\n",
    "2. Map each number to a `(1, (number, 1))` pair.\n",
    "3. Use `reduceByKey` to sum up both the values and counts.\n",
    "4. Calculate the average by dividing the total sum by the count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def compute_average(sc, numbers):\n",
    "    # Parallelize the input list into an RDD\n",
    "    rdd = sc.parallelize(numbers)\n",
    "    \n",
    "    # Map each number to a (number, 1) pair for sum and count\n",
    "    rdd_mapped = rdd.map(lambda x: (1, (x, 1)))\n",
    "    \n",
    "    # Reduce by summing up the values and counts\n",
    "    reduced_rdd = rdd_mapped.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "    # Compute the average by dividing sum by count\n",
    "    avg = reduced_rdd.mapValues(lambda x: x[0] / x[1])\n",
    "    \n",
    "    # Collect the result\n",
    "    result = avg.collect()\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    result = compute_average(sc, numbers)\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}